# BI Dashboard Backend

ä¸€å€‹åŸºæ–¼ .NET 8 çš„å•†æ¥­æ™ºæ…§å„€è¡¨æ¿å¾Œç«¯æœå‹™ï¼Œæä¾›è³‡æ–™è™•ç†ã€æŒ‡æ¨™è¨ˆç®—å’Œ API æœå‹™ã€‚

## ğŸš€ æŠ€è¡“æ£§

- **.NET 8** - ä¸»è¦é–‹ç™¼æ¡†æ¶
- **ASP.NET Core** - Web API æ¡†æ¶
- **PostgreSQL** - ä¸»è¦è³‡æ–™åº«
- **Redis** - å¿«å–æœå‹™
- **Hangfire** - èƒŒæ™¯å·¥ä½œæ’ç¨‹
- **Dapper** - è¼•é‡ç´š ORM
- **Serilog** - çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„
- **JWT** - èº«ä»½é©—è­‰
- **Swagger** - API æ–‡ä»¶

## ğŸ“‹ ç³»çµ±éœ€æ±‚

- .NET 8 SDK
- PostgreSQL 12+
- Redis 6+
- Docker (å¯é¸ï¼Œç”¨æ–¼å®¹å™¨åŒ–éƒ¨ç½²)

## ğŸ› ï¸ ç’°å¢ƒè¨­ç½®

### 1. å…‹éš†å°ˆæ¡ˆ

```bash
git clone https://github.com/austin72905/BIDashboard.git
cd BIDashboardBackend
```

### 2. è³‡æ–™åº«è¨­ç½®

#### PostgreSQL è¨­ç½®

```bash
# ä½¿ç”¨ Docker å•Ÿå‹• PostgreSQL
docker run --name bi-postgres \
  -e POSTGRES_DB=bidb \
  -e POSTGRES_USER=admin \
  -e POSTGRES_PASSWORD=admin123 \
  -p 5432:5432 \
  -d postgres:15



#### Redis è¨­ç½®

```bash
# ä½¿ç”¨ Docker å•Ÿå‹• Redis
docker run --name bi-redis \
  -p 6379:6379 \
  -d redis:7-alpine



### 3. è³‡æ–™åº«åˆå§‹åŒ–

åŸ·è¡Œå°ˆæ¡ˆä¸­çš„ SQL è…³æœ¬å»ºç«‹å¿…è¦çš„è³‡æ–™è¡¨å’Œå‡½æ•¸ï¼š

```bash
# é€£æ¥åˆ° PostgreSQL ä¸¦åŸ·è¡Œåˆå§‹åŒ–è…³æœ¬
psql -h localhost -U admin -d bidb -f scripts/schema.sql
```



## ğŸ“š API æ–‡ä»¶

å•Ÿå‹•å°ˆæ¡ˆå¾Œï¼Œå¯é€éä»¥ä¸‹ç¶²å€æŸ¥çœ‹ API æ–‡ä»¶ï¼š

- Swagger UI: `http://localhost:5127/swagger`
- OpenAPI JSON: `http://localhost:5127/swagger/v1/swagger.json`

## ğŸ”§ ä¸»è¦åŠŸèƒ½

### è³‡æ–™è™•ç†
- **æª”æ¡ˆä¸Šå‚³**: æ”¯æ´ CSV æª”æ¡ˆä¸Šå‚³å’Œè§£æ
- **æ¬„ä½æ˜ å°„**: å‹•æ…‹æ¬„ä½æ˜ å°„é…ç½®
- **æ‰¹æ¬¡è™•ç†**: ä½¿ç”¨ Hangfire é€²è¡ŒèƒŒæ™¯æ‰¹æ¬¡è™•ç†
- **ETL æµç¨‹**: å®Œæ•´çš„ Extract-Transform-Load æµç¨‹

### æŒ‡æ¨™è¨ˆç®—
- **KPI æ‘˜è¦**: ç¸½ç‡Ÿæ”¶ã€å®¢æˆ¶æ•¸ã€è¨‚å–®æ•¸ç­‰é—œéµæŒ‡æ¨™
- **è¶¨å‹¢åˆ†æ**: æœˆç‡Ÿæ”¶è¶¨å‹¢ã€å®¢æˆ¶å¢é•·è¶¨å‹¢
- **åˆ†å¸ƒçµ±è¨ˆ**: åœ°å€åˆ†å¸ƒã€å¹´é½¡åˆ†å¸ƒã€æ€§åˆ¥åˆ†å¸ƒ
- **ç”¢å“åˆ†æ**: ç”¢å“é¡åˆ¥éŠ·å”®çµ±è¨ˆ

### å¿«å–æ©Ÿåˆ¶
- **Redis å¿«å–**: æå‡æŸ¥è©¢æ•ˆèƒ½
- **æ™ºæ…§å¿«å–**: è‡ªå‹•å¿«å–å¤±æ•ˆå’Œæ›´æ–°
- **å¿«å–ç›£æ§**: å¿«å–å‘½ä¸­ç‡è¨˜éŒ„

### æ—¥èªŒè¨˜éŒ„
- **çµæ§‹åŒ–æ—¥èªŒ**: ä½¿ç”¨ Serilog è¨˜éŒ„çµæ§‹åŒ–æ—¥èªŒ
- **å¤šå±¤ç´šè¨˜éŒ„**: Debugã€Infoã€Warningã€Error
- **æª”æ¡ˆè¼ªæ›¿**: è‡ªå‹•æ—¥èªŒæª”æ¡ˆè¼ªæ›¿å’Œæ¸…ç†
- **æ•ˆèƒ½ç›£æ§**: è¨˜éŒ„æŸ¥è©¢è€—æ™‚å’Œå¿«å–ç‹€æ…‹

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

### æ•´é«”æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph "Frontend"
        A[React Dashboard<br/>Port: 5173]
    end
    
    subgraph "Backend Services"
        B[.NET 8 API<br/>Port: 5127]
        C[Hangfire Dashboard<br/>Background Jobs]
    end
    
    subgraph "Data Layer"
        D[PostgreSQL<br/>Port: 5432]
        E[Redis Cache<br/>Port: 6379]
    end
    
    A -->|HTTP/HTTPS| B
    B -->|SQL Queries| D
    B -->|Cache Operations| E
    C -->|ETL Processing| D
    C -->|Cache Updates| E
```

### å¾Œç«¯æœå‹™æ¶æ§‹

```mermaid
graph TB
    subgraph "API Layer"
        A1[Auth Controller]
        A2[Metrics Controller]
        A3[Uploads Controller]
    end
    
    subgraph "Business Layer"
        B1[Auth Service]
        B2[Metric Service]
        B3[Dataset Service]
    end
    
    subgraph "Data Access Layer"
        C1[User Repository]
        C2[Metric Repository]
        C3[Dataset Repository]
    end
    
    subgraph "Background Processing"
        D1[ETL Job]
        D2[Batch Processing]
    end
    
    subgraph "External Services"
        E1[PostgreSQL]
        E2[Redis Cache]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C2
    B3 --> C3
    
    C1 --> E1
    C2 --> E1
    C3 --> E1
    
    C2 --> E2
    B2 --> E2
    
    D1 --> E1
    D1 --> E2
    D2 --> D1
```

### è³‡æ–™æµç¨‹åœ–

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant H as Hangfire
    participant P as PostgreSQL
    participant R as Redis
    
    U->>F: Upload CSV
    F->>B: POST /api/uploads
    B->>P: Store Dataset
    B->>H: Queue ETL Job
    H->>P: Process Data
    H->>P: Calculate Metrics
    H->>R: Update Cache
    B->>F: Return Success
    F->>B: GET /api/metrics
    B->>R: Check Cache
    alt Cache Hit
        R->>B: Return Cached Data
    else Cache Miss
        B->>P: Query Database
        P->>B: Return Data
        B->>R: Update Cache
    end
    B->>F: Return Metrics
    F->>U: Display Dashboard
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
BIDashboardBackend/
â”œâ”€â”€ source/
â”‚   â””â”€â”€ BIDashboardBackend/
â”‚       â”œâ”€â”€ Controllers/          # API æ§åˆ¶å™¨
â”‚       â”œâ”€â”€ Services/            # æ¥­å‹™é‚è¼¯æœå‹™
â”‚       â”œâ”€â”€ Repositories/        # è³‡æ–™å­˜å–å±¤
â”‚       â”œâ”€â”€ Features/            # åŠŸèƒ½æ¨¡çµ„
â”‚       â”‚   â”œâ”€â”€ Jobs/           # Hangfire èƒŒæ™¯å·¥ä½œ
â”‚       â”‚   â””â”€â”€ Ingest/         # è³‡æ–™æ”å–
â”‚       â”œâ”€â”€ Models/             # è³‡æ–™æ¨¡å‹
â”‚       â”œâ”€â”€ DTOs/               # è³‡æ–™å‚³è¼¸ç‰©ä»¶
â”‚       â”œâ”€â”€ Interfaces/         # ä»‹é¢å®šç¾©
â”‚       â”œâ”€â”€ Configs/            # é…ç½®é¡åˆ¥
â”‚       â”œâ”€â”€ Caching/            # å¿«å–æœå‹™
â”‚       â”œâ”€â”€ Database/           # è³‡æ–™åº«ç›¸é—œ
â”‚       â””â”€â”€ Utils/              # å·¥å…·é¡åˆ¥
â”œâ”€â”€ tests/                      # å–®å…ƒæ¸¬è©¦
â”œâ”€â”€ docs/                       # æ–‡ä»¶
â”‚   â””â”€â”€ images/                 # æ¶æ§‹åœ–å’Œèªªæ˜åœ–ç‰‡
â”œâ”€â”€ scripts/                    # è³‡æ–™åº«è…³æœ¬
â””â”€â”€ Dockerfile                  # Docker é…ç½®
```

## ğŸ—„ï¸ è³‡æ–™åº«è¨­è¨ˆ

### æ ¸å¿ƒè¨­è¨ˆç†å¿µ

æœ¬ç³»çµ±æ¡ç”¨**å…©éšæ®µèšåˆ**çš„è¨­è¨ˆæ¨¡å¼ï¼Œå…ˆè¨ˆç®—æ¯å€‹æ‰¹æ¬¡çš„ä¸­é–“çµ±è¨ˆå€¼ï¼Œå†èšåˆç‚ºæœ€çµ‚çµ±è¨ˆè¡¨ï¼Œä»¥é”åˆ°é«˜æ•ˆèƒ½å’Œè³‡æ–™ä¸€è‡´æ€§ã€‚

### ä¸»è¦è³‡æ–™è¡¨

#### 1. è³‡æ–™é›†ç›¸é—œè¡¨

```sql
-- è³‡æ–™é›†ä¸»è¡¨
datasets (id, name, owner_id, created_at, updated_at)

-- è³‡æ–™é›†æ‰¹æ¬¡è¡¨
dataset_batches (
    id, dataset_id, source_filename, total_rows, 
    status, error_message, created_at, updated_at
)

-- è³‡æ–™é›†æ¬„ä½æ˜ å°„è¡¨
dataset_mappings (
    id, batch_id, source_column, system_field, 
    created_at, updated_at
)

-- åŸå§‹è³‡æ–™è¡Œè¡¨
dataset_rows (
    id, batch_id, row_json, created_at
)
```

#### 2. çµ±è¨ˆå€¼è¡¨ï¼ˆå…©éšæ®µèšåˆï¼‰

```sql
-- æ‰¹æ¬¡ç´šåˆ¥çµ±è¨ˆè¡¨ï¼ˆä¸­é–“çµ±è¨ˆå€¼ï¼‰
materialized_metrics_by_batch (
    id, dataset_id, batch_id, metric_key, bucket, period,
    sum_value, count_value, updated_at
)

-- æœ€çµ‚çµ±è¨ˆè¡¨ï¼ˆèšåˆçµæœï¼‰
materialized_metrics (
    id, dataset_id, metric_key, bucket, period,
    value, updated_at
)
```

### å…©éšæ®µèšåˆæµç¨‹

```mermaid
graph TB
    A[CSV æª”æ¡ˆä¸Šå‚³] --> B[å»ºç«‹ Dataset Batch]
    B --> C[æ¬„ä½æ˜ å°„é…ç½®]
    C --> D[ETL Job è§¸ç™¼]
    
    D --> E[éšæ®µä¸€ï¼šæ‰¹æ¬¡çµ±è¨ˆ]
    E --> F[fn_mm_insert_metrics_by_batch]
    F --> G[materialized_metrics_by_batch]
    
    G --> H[éšæ®µäºŒï¼šæœ€çµ‚èšåˆ]
    H --> I[fn_mm_upsert_final_metrics]
    I --> J[materialized_metrics]
    
    J --> K[æ¸…é™¤å¿«å–]
    K --> L[API å›æ‡‰]
```

### æ‰¹æ¬¡è³‡æ–™è½‰æ›ä¸­é–“è¡¨è™•ç†æµç¨‹

æœ¬ç³»çµ±è¨­è¨ˆçš„æ ¸å¿ƒå„ªå‹¢åœ¨æ–¼**å¢é‡è™•ç†èƒ½åŠ›**ï¼šç•¶æŸå€‹ batch çš„ mapping ç™¼ç”Ÿè®Šæ›´æ™‚ï¼Œä¸éœ€è¦é‡æ–°è¨ˆç®—æ•´å€‹ datasetï¼Œåªéœ€é‡æ–°è™•ç†è©²æ‰¹æ¬¡ä¸¦æ›´æ–°æœ€çµ‚çµ±è¨ˆã€‚

#### 1. æ¬„ä½æ˜ å°„ (Mapping) éšæ®µ

```mermaid
graph LR
    A[åŸå§‹ CSV æ¬„ä½] --> B[Dataset Mappings è¡¨]
    B --> C[SystemField æ¨™æº–åŒ–]
    
    subgraph "SystemField åˆ—èˆ‰"
        D[Name=0, Email=1, Phone=2<br/>Gender=3, BirthDate=4, Age=5<br/>CustomerId=6, OrderId=7<br/>OrderDate=8, OrderAmount=9<br/>OrderStatus=10, Region=11<br/>ProductCategory=12]
    end
    
    C --> D
```

æ¯å€‹æ‰¹æ¬¡çš„åŸå§‹æ¬„ä½é€é `dataset_mappings` è¡¨å°æ‡‰åˆ°æ¨™æº–åŒ–çš„ `SystemField`ï¼š

```sql
-- æ¬„ä½æ˜ å°„ç¯„ä¾‹
INSERT INTO dataset_mappings (batch_id, source_column, system_field) VALUES
(1, 'å®¢æˆ¶å§“å', 0),      -- Name
(1, 'æ€§åˆ¥', 3),          -- Gender  
(1, 'å¹´é½¡', 5),          -- Age
(1, 'è¨‚å–®æ—¥æœŸ', 8),      -- OrderDate
(1, 'è¨‚å–®é‡‘é¡', 9),      -- OrderAmount
(1, 'åœ°å€', 11);         -- Region
```

#### 2. ä¸­é–“è¡¨è½‰æ›æµç¨‹

```mermaid
graph TB
    subgraph "åŸå§‹è³‡æ–™å±¤"
        A[dataset_rows<br/>JSON æ ¼å¼åŸå§‹è³‡æ–™]
    end
    
    subgraph "è½‰æ›è™•ç†"
        B[ä¾æ“š mapping æå–æ¬„ä½]
        C[è³‡æ–™å‹åˆ¥è½‰æ›èˆ‡é©—è­‰]
        D[è¡ç”Ÿæ¬„ä½è¨ˆç®—]
    end
    
    subgraph "ä¸­é–“çµ±è¨ˆå±¤"
        E[materialized_metrics_by_batch<br/>æ‰¹æ¬¡ç´šåˆ¥çµ±è¨ˆå€¼]
    end
    
    subgraph "æœ€çµ‚çµ±è¨ˆå±¤"  
        F[materialized_metrics<br/>èšåˆå¾Œæœ€çµ‚å€¼]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    
    style E fill:#e1f5fe
    style F fill:#c8e6c9
```

#### 3. è³‡æ–™å‹åˆ¥è½‰æ›èˆ‡é©—è­‰

ç³»çµ±æœƒä¾æ“š mapping å¾ JSON æ ¼å¼çš„åŸå§‹è³‡æ–™ä¸­æå–å°æ‡‰æ¬„ä½ï¼Œä¸¦é€²è¡Œåš´æ ¼çš„è³‡æ–™é©—è­‰ï¼š

```sql
-- æå–ä¸¦è½‰æ›è³‡æ–™çš„æ ¸å¿ƒé‚è¼¯ï¼ˆæ‘˜è‡ª fn_mm_insert_metrics_by_batchï¼‰
CREATE TEMP TABLE _rows_raw AS
SELECT
  (r.row_json ->> (SELECT column_name FROM _map WHERE system_field = 8)) AS order_date_txt,
  (r.row_json ->> (SELECT column_name FROM _map WHERE system_field = 9)) AS order_amount_txt,
  (r.row_json ->> (SELECT column_name FROM _map WHERE system_field = 3)) AS gender_txt,
  -- ... å…¶ä»–æ¬„ä½
FROM dataset_rows r
WHERE r.batch_id = p_batch_id;

-- è³‡æ–™æ­£è¦åŒ–èˆ‡é©—è­‰
CREATE TEMP TABLE _norm AS  
SELECT
  -- å®‰å…¨çš„æ—¥æœŸè½‰æ›ï¼ˆä½¿ç”¨è‡ªå®šç¾©å‡½æ•¸ï¼‰
  safe_date_convert(order_date_txt) AS order_date,
  
  -- æ•¸å€¼é©—è­‰ï¼ˆåƒ…æ¥å—æœ‰æ•ˆæ•¸å­—æ ¼å¼ï¼‰
  CASE WHEN order_amount_txt ~ '^\s*-?\d+(\.\d+)?\s*$' 
       THEN TRIM(order_amount_txt)::NUMERIC 
  END AS order_amount,
  
  -- å¹´é½¡ç¯„åœé™åˆ¶ï¼ˆ0-100æ­²ï¼‰
  CASE WHEN age_txt ~ '^[0-9]+$' AND age_txt::INT BETWEEN 0 AND 100
       THEN TRIM(age_txt)::INT 
  END AS age,
  
  -- è¨ˆç®—è¡ç”Ÿæ¬„ä½ï¼šæœˆä»½æœŸé–“
  COALESCE(date_trunc('month', order_date)::DATE, '1900-01-01') AS period_month,
  
  -- è¨ˆç®—è¡ç”Ÿæ¬„ä½ï¼šå¹´é½¡å€é–“ 
  CASE WHEN age IS NOT NULL 
       THEN CONCAT((age/10)*10, '-', (age/10)*10 + 9) 
  END AS age_bucket
FROM _rows_raw;
```

#### 4. ä¸­é–“çµ±è¨ˆå€¼è¨ˆç®—

ç³»çµ±æœƒé‡å°ä¸åŒé¡å‹çš„æŒ‡æ¨™è¨ˆç®—ä¸­é–“çµ±è¨ˆå€¼ï¼Œå„²å­˜åœ¨ `materialized_metrics_by_batch` è¡¨ä¸­ï¼š

```sql
-- ç¯„ä¾‹ï¼šè¨ˆç®—æ¯æœˆç‡Ÿæ”¶çµ±è¨ˆ
INSERT INTO materialized_metrics_by_batch 
  (dataset_id, batch_id, metric_key, bucket, period, sum_value)
SELECT dataset_id, batch_id, 0, NULL, period_month, SUM(order_amount)
FROM _norm
WHERE order_amount IS NOT NULL
GROUP BY dataset_id, batch_id, period_month;

-- ç¯„ä¾‹ï¼šè¨ˆç®—æ€§åˆ¥åˆ†å¸ƒçµ±è¨ˆ
INSERT INTO materialized_metrics_by_batch
  (dataset_id, batch_id, metric_key, bucket, period, count_value)
SELECT dataset_id, batch_id, 12, gender, v_sentinel_period, COUNT(*)
FROM _norm
WHERE gender IS NOT NULL
GROUP BY dataset_id, batch_id, gender;
```

#### 5. ä¾æ“šä¸åŒ Period çµ±è¨ˆ

ç³»çµ±æ”¯æ´å¤šç¨®æ™‚é–“ç¶­åº¦çš„çµ±è¨ˆï¼š

| Period é¡å‹ | èªªæ˜ | æ‡‰ç”¨å ´æ™¯ |
|------------|------|----------|
| **æœˆä»½æœŸé–“** | `period_month` | ç‡Ÿæ”¶è¶¨å‹¢ã€è¨‚å–®é‡è®ŠåŒ– |
| **å“¨å…µæœŸé–“** | `1900-01-01` | ä¸ä¾æ™‚é–“çš„åˆ†å¸ƒçµ±è¨ˆï¼ˆæ€§åˆ¥ã€å¹´é½¡ã€åœ°å€ï¼‰ |
| **è‡ªå®šç¾©æœŸé–“** | å¯æ“´å±•æ”¯æ´å­£åº¦ã€å¹´åº¦ | é•·æœŸè¶¨å‹¢åˆ†æ |

```sql
-- æ™‚é–“ç›¸é—œæŒ‡æ¨™ï¼ˆæŒ‰æœˆçµ±è¨ˆï¼‰
INSERT INTO materialized_metrics_by_batch 
  (dataset_id, batch_id, metric_key, period, sum_value)
SELECT dataset_id, batch_id, 0, period_month, SUM(order_amount)
FROM _norm 
WHERE period_month IS NOT NULL
GROUP BY dataset_id, batch_id, period_month;

-- åˆ†å¸ƒé¡æŒ‡æ¨™ï¼ˆä½¿ç”¨å“¨å…µæœŸé–“ï¼Œä¸æŒ‰æ™‚é–“åˆ†çµ„ï¼‰
INSERT INTO materialized_metrics_by_batch
  (dataset_id, batch_id, metric_key, bucket, period, count_value)  
SELECT dataset_id, batch_id, 12, gender, DATE '1900-01-01', COUNT(*)
FROM _norm
WHERE gender IS NOT NULL
GROUP BY dataset_id, batch_id, gender;
```

### çµ±è¨ˆæŒ‡æ¨™é¡å‹

ç³»çµ±æ”¯æ´ä»¥ä¸‹çµ±è¨ˆæŒ‡æ¨™ï¼š

| æŒ‡æ¨™é¡å‹ | MetricKey | èªªæ˜ | èšåˆæ–¹å¼ |
|---------|-----------|------|----------|
| ç¸½ç‡Ÿæ”¶ | TotalRevenue | æ‰€æœ‰è¨‚å–®çš„ç¸½é‡‘é¡ | SUM |
| ç¸½å®¢æˆ¶æ•¸ | TotalCustomers | ä¸é‡è¤‡å®¢æˆ¶æ•¸é‡ | COUNT DISTINCT |
| ç¸½è¨‚å–®æ•¸ | TotalOrders | è¨‚å–®ç¸½ç­†æ•¸ | COUNT |
| å¹³å‡è¨‚å–®é‡‘é¡ | AvgOrderValue | ç¸½ç‡Ÿæ”¶/ç¸½è¨‚å–®æ•¸ | SUM/COUNT |
| æ–°å®¢æˆ¶æ•¸ | NewCustomers | æŒ‡å®šæœŸé–“å…§çš„æ–°å®¢æˆ¶ | COUNT |
| æœˆç‡Ÿæ”¶è¶¨å‹¢ | MonthlyRevenueTrend | æŒ‰æœˆåˆ†çµ„çš„ç‡Ÿæ”¶ | SUM by Period |
| åœ°å€åˆ†å¸ƒ | RegionDistribution | å„åœ°å€çš„è¨‚å–®åˆ†å¸ƒ | COUNT by Region |
| ç”¢å“é¡åˆ¥éŠ·å”® | ProductCategorySales | å„é¡åˆ¥ç”¢å“éŠ·é‡ | COUNT by Category |
| å¹´é½¡åˆ†å¸ƒ | AgeDistribution | å®¢æˆ¶å¹´é½¡åˆ†å¸ƒ | COUNT by Age Group |
| æ€§åˆ¥å æ¯” | GenderShare | å®¢æˆ¶æ€§åˆ¥åˆ†å¸ƒ | COUNT by Gender |

### æ‰¹æ¬¡è™•ç†ç‹€æ…‹

```mermaid
stateDiagram-v2
    [*] --> Pending: æª”æ¡ˆä¸Šå‚³
    Pending --> Mapped: æ¬„ä½æ˜ å°„å®Œæˆ
    Mapped --> Processing: ETL Job é–‹å§‹
    Processing --> Succeeded: è™•ç†æˆåŠŸ
    Processing --> Failed: è™•ç†å¤±æ•—
    Failed --> Processing: é‡æ–°è™•ç†
    Succeeded --> [*]
```

### å¢é‡æ›´æ–°æ©Ÿåˆ¶ï¼šå–®å€‹ Batch è®Šæ›´ Mapping çš„è™•ç†

æœ¬ç³»çµ±çš„æ ¸å¿ƒå„ªå‹¢åœ¨æ–¼æ”¯æ´**ç´°ç²’åº¦çš„å¢é‡æ›´æ–°**ã€‚ç•¶æŸå€‹ batch çš„ mapping é…ç½®ç™¼ç”Ÿè®Šæ›´æ™‚ï¼Œç³»çµ±åªéœ€é‡æ–°è™•ç†è©²æ‰¹æ¬¡ï¼Œè€Œç„¡éœ€é‡æ–°è¨ˆç®—æ•´å€‹ dataset çš„æ‰€æœ‰çµ±è¨ˆå€¼ã€‚

#### å¢é‡æ›´æ–°æµç¨‹åœ–

```mermaid
sequenceDiagram
    participant U as ä½¿ç”¨è€…
    participant B as Backend API  
    participant H as Hangfire Jobs
    participant P as PostgreSQL
    participant R as Redis Cache
    
    U->>B: ä¿®æ”¹ Batch Mapping
    B->>P: æ›´æ–° dataset_mappings
    B->>H: è§¸ç™¼å¢é‡è™•ç† Job
    
    Note over H,P: æ­¥é©Ÿ1ï¼šæ¸…ç†è©²æ‰¹æ¬¡çš„èˆŠçµ±è¨ˆ
    H->>P: fn_mm_clear_old_by_batch(dataset_id, batch_id)
    P->>P: DELETE FROM materialized_metrics_by_batch<br/>WHERE dataset_id=X AND batch_id=Y
    
    Note over H,P: æ­¥é©Ÿ2ï¼šé‡æ–°è¨ˆç®—è©²æ‰¹æ¬¡çµ±è¨ˆ
    H->>P: fn_mm_insert_metrics_by_batch(dataset_id, batch_id)
    P->>P: åŸºæ–¼æ–° mapping é‡æ–°è¨ˆç®—<br/>INSERT INTO materialized_metrics_by_batch
    
    Note over H,P: æ­¥é©Ÿ3ï¼šæ›´æ–°æœ€çµ‚çµ±è¨ˆè¡¨ï¼ˆåƒ…å—å½±éŸ¿éƒ¨åˆ†ï¼‰
    H->>P: sp_mm_upsert_final_for_affected(dataset_id, batch_id)
    P->>P: æ‰¾å‡ºå—å½±éŸ¿çš„æŒ‡æ¨™åˆ‡ç‰‡
    P->>P: é‡æ–°èšåˆä¸¦æ›´æ–° materialized_metrics
    
    Note over H,R: æ­¥é©Ÿ4ï¼šæ¸…é™¤ç›¸é—œå¿«å–
    H->>R: åˆªé™¤è©² dataset ç›¸é—œå¿«å–
    
    H->>B: è™•ç†å®Œæˆ
    B->>U: å›æ‡‰æˆåŠŸ
```

#### é—œéµæŠ€è¡“å¯¦ç¾

##### 1. æ‰¹æ¬¡ç´šåˆ¥çš„ç¨ç«‹è™•ç†

æ¯å€‹ batch çš„çµ±è¨ˆè³‡æ–™å®Œå…¨ç¨ç«‹å„²å­˜åœ¨ `materialized_metrics_by_batch` è¡¨ä¸­ï¼š

```sql
-- æ¸…é™¤æŒ‡å®šæ‰¹æ¬¡çš„èˆŠçµ±è¨ˆï¼ˆé¿å…é‡è¤‡ç´¯è¨ˆï¼‰
CREATE FUNCTION fn_mm_clear_old_by_batch(p_dataset_id bigint, p_batch_id bigint)
RETURNS void AS $$
BEGIN
  DELETE FROM materialized_metrics_by_batch
  WHERE dataset_id = p_dataset_id AND batch_id = p_batch_id;
END;
$$;
```

##### 2. å½±éŸ¿ç¯„åœç²¾ç¢ºå®šä½

ç³»çµ±èƒ½ç²¾ç¢ºæ‰¾å‡ºå—å–®ä¸€ batch è®Šæ›´å½±éŸ¿çš„çµ±è¨ˆåˆ‡ç‰‡ï¼š

```sql
-- æ‰¾å‡ºå—å½±éŸ¿çš„æŒ‡æ¨™åˆ‡ç‰‡ï¼ˆæ‘˜è‡ª sp_mm_upsert_final_for_affectedï¼‰
CREATE TEMP TABLE _affected AS
SELECT DISTINCT dataset_id, metric_key, bucket, period
FROM materialized_metrics_by_batch
WHERE dataset_id = p_dataset_id AND batch_id = p_batch_id;
```

##### 3. æ™ºæ…§èšåˆç­–ç•¥

é‡å°ä¸åŒé¡å‹çš„æŒ‡æ¨™æ¡ç”¨å°æ‡‰çš„èšåˆç­–ç•¥ï¼š

```sql
-- Sum é¡å‹æŒ‡æ¨™ï¼ˆç‡Ÿæ”¶ã€éŠ·å”®é¡ï¼‰
SELECT SUM(sum_value) FROM materialized_metrics_by_batch 
WHERE dataset_id = ? AND metric_key IN (0, 8, 9)
GROUP BY dataset_id, metric_key, bucket, period;

-- Count é¡å‹æŒ‡æ¨™ï¼ˆè¨‚å–®æ•¸ã€å®¢æˆ¶æ•¸ï¼‰  
SELECT SUM(count_value) FROM materialized_metrics_by_batch
WHERE dataset_id = ? AND metric_key IN (1, 2, 6, 11)
GROUP BY dataset_id, metric_key, bucket, period;

-- Average é¡å‹æŒ‡æ¨™ï¼ˆå¹³å‡è¨‚å–®é‡‘é¡ï¼‰
SELECT SUM(sum_value) / SUM(count_value) FROM materialized_metrics_by_batch
WHERE dataset_id = ? AND metric_key = 3
GROUP BY dataset_id, metric_key, bucket, period;

-- Share é¡å‹æŒ‡æ¨™ï¼ˆä½”æ¯”åˆ†å¸ƒï¼‰
SELECT count_value / SUM(count_value) OVER (PARTITION BY dataset_id, metric_key, period)
FROM materialized_metrics_by_batch
WHERE dataset_id = ? AND metric_key IN (10, 12);
```

#### å¢é‡æ›´æ–°çš„å„ªå‹¢

| å‚³çµ±å…¨é‡æ›´æ–° | æœ¬ç³»çµ±å¢é‡æ›´æ–° |
|-------------|----------------|
| é‡æ–°è™•ç†æ•´å€‹ dataset | åªè™•ç†è®Šæ›´çš„ batch |
| è™•ç†æ™‚é–“éš¨è³‡æ–™é‡ç·šæ€§å¢é•· | è™•ç†æ™‚é–“å›ºå®šï¼ˆå–®æ‰¹æ¬¡ï¼‰ |
| éœ€è¦å¤§é‡ç³»çµ±è³‡æº | è³‡æºæ¶ˆè€—æœ€å°åŒ– |
| è™•ç†æœŸé–“å½±éŸ¿å…¶ä»–æŸ¥è©¢ | å½±éŸ¿ç¯„åœæœ€å° |
| å¤±æ•—éœ€å…¨éƒ¨é‡ä¾† | å¤±æ•—åªå½±éŸ¿å–®ä¸€æ‰¹æ¬¡ |

#### å¯¦éš›æ‡‰ç”¨å ´æ™¯

```mermaid
graph TB
    subgraph "å ´æ™¯ï¼šä¿®æ­£æ‰¹æ¬¡ A çš„æ€§åˆ¥æ¬„ä½æ˜ å°„"
        A1[åŸæ˜ å°„: 'Sex' â†’ Gender] --> A2[æ–°æ˜ å°„: 'Gender' â†’ Gender]
        A2 --> A3[è§¸ç™¼å¢é‡æ›´æ–°]
    end
    
    subgraph "è™•ç†ç¯„åœ"
        B1[åªé‡æ–°è™•ç†æ‰¹æ¬¡ A]
        B2[å…¶ä»–æ‰¹æ¬¡ Bã€Cã€D ä¸å—å½±éŸ¿]
        B3[åªæ›´æ–°æ€§åˆ¥ç›¸é—œçµ±è¨ˆåˆ‡ç‰‡]
    end
    
    subgraph "æœ€çµ‚çµæœ"
        C1[æ€§åˆ¥åˆ†å¸ƒçµ±è¨ˆæ›´æ–°]
        C2[å…¶ä»–çµ±è¨ˆä¿æŒä¸è®Š]
        C3[æ•´é«”æ•ˆèƒ½å½±éŸ¿æœ€å°]
    end
    
    A3 --> B1
    B1 --> C1
```

### æ•ˆèƒ½å„ªåŒ–ç­–ç•¥

#### 1. å¢é‡æ›´æ–°
- **æ‰¹æ¬¡ç´šåˆ¥ç¨ç«‹æ€§**ï¼šæ¯å€‹ batch çµ±è¨ˆå®Œå…¨ç¨ç«‹
- **ç²¾ç¢ºå½±éŸ¿ç¯„åœ**ï¼šåªæ›´æ–°å—è®Šæ›´å½±éŸ¿çš„çµ±è¨ˆåˆ‡ç‰‡
- **æ™ºæ…§èšåˆ**ï¼šä¾æŒ‡æ¨™é¡å‹æ¡ç”¨æœ€ä½³èšåˆç­–ç•¥

#### 2. å¿«å–æ©Ÿåˆ¶
- **åˆ†å±¤å¿«å–**ï¼šæ‰¹æ¬¡ç´šåˆ¥ + æœ€çµ‚çµ±è¨ˆé›™å±¤å¿«å–
- **ç²¾ç¢ºå¤±æ•ˆ**ï¼šåªæ¸…é™¤å—å½±éŸ¿ dataset çš„ç›¸é—œå¿«å–
- **é ç†±ç­–ç•¥**ï¼šè™•ç†å®Œæˆå¾Œä¸»å‹•é ç†±å¸¸ç”¨æŸ¥è©¢

#### 3. ä¸¦è¡Œè™•ç†
- **æ‰¹æ¬¡ä¸¦è¡Œ**ï¼šå¤šå€‹æ‰¹æ¬¡å¯åŒæ™‚è™•ç†ï¼ˆä¸åŒ datasetï¼‰
- **æŒ‡æ¨™ä¸¦è¡Œ**ï¼šåŒä¸€æ‰¹æ¬¡å…§ä¸åŒæŒ‡æ¨™å¯ä¸¦è¡Œè¨ˆç®—
- **èšåˆä¸¦è¡Œ**ï¼šæœ€çµ‚èšåˆéšæ®µæ”¯æ´ä¸¦è¡Œæ›´æ–°

#### 4. ç´¢å¼•è¨­è¨ˆ
```sql
-- æ‰¹æ¬¡çµ±è¨ˆè¡¨ç´¢å¼•ï¼ˆæ”¯æ´å¿«é€Ÿå®šä½å—å½±éŸ¿åˆ‡ç‰‡ï¼‰
CREATE INDEX idx_mmbb_dataset_metric_bucket_period 
ON materialized_metrics_by_batch(dataset_id, metric_key, bucket, period);

-- æœ€çµ‚çµ±è¨ˆè¡¨ç´¢å¼•ï¼ˆæ”¯æ´å¿«é€ŸæŸ¥è©¢å’Œæ›´æ–°ï¼‰
CREATE INDEX idx_mm_dataset_metric_bucket 
ON materialized_metrics(dataset_id, metric_key, bucket, period);

-- æ‰¹æ¬¡æŸ¥è©¢ç´¢å¼•ï¼ˆæ”¯æ´å½±éŸ¿ç¯„åœåˆ†æï¼‰
CREATE INDEX idx_mmbb_batch 
ON materialized_metrics_by_batch(batch_id);
```

#### 5. å¯¦éš›ç¨‹å¼ç¢¼ç¯„ä¾‹

åœ¨ C# å¾Œç«¯æœå‹™ä¸­ï¼Œå¢é‡æ›´æ–°çš„è§¸ç™¼æµç¨‹å¦‚ä¸‹ï¼š

```csharp
// ç•¶ä½¿ç”¨è€…ä¿®æ”¹æ‰¹æ¬¡æ˜ å°„æ™‚è§¸ç™¼
public async Task<bool> UpdateBatchMappingAsync(long batchId, List<DatasetMappingDto> newMappings)
{
    using var transaction = await _unitOfWork.BeginTransactionAsync();
    try
    {
        // 1. æ›´æ–°æ˜ å°„é…ç½®
        await _datasetRepository.UpdateBatchMappingsAsync(batchId, newMappings);
        
        // 2. å–å¾—æ‰¹æ¬¡è³‡è¨Š
        var batch = await _datasetRepository.GetBatchAsync(batchId);
        
        // 3. è§¸ç™¼å¢é‡æ›´æ–° Jobï¼ˆéåŒæ­¥èƒŒæ™¯è™•ç†ï¼‰
        _backgroundJobClient.Enqueue<IncrementalUpdateJob>(
            job => job.ProcessSingleBatchAsync(batch.DatasetId, batchId));
        
        // 4. æ›´æ–°æ‰¹æ¬¡ç‹€æ…‹
        await _datasetRepository.UpdateBatchStatusAsync(batchId, "Processing");
        
        await transaction.CommitAsync();
        return true;
    }
    catch (Exception ex)
    {
        await transaction.RollbackAsync();
        _logger.LogError(ex, "æ›´æ–°æ‰¹æ¬¡æ˜ å°„å¤±æ•—: BatchId={BatchId}", batchId);
        return false;
    }
}

// Hangfire èƒŒæ™¯å·¥ä½œï¼šå¢é‡æ›´æ–°è™•ç†
public class IncrementalUpdateJob
{
    public async Task ProcessSingleBatchAsync(long datasetId, long batchId)
    {
        try
        {
            // 1. æ¸…ç†è©²æ‰¹æ¬¡çš„èˆŠçµ±è¨ˆ
            await _sqlRunner.ExecuteAsync(
                "SELECT fn_mm_clear_old_by_batch(@datasetId, @batchId)",
                new { datasetId, batchId });
            
            // 2. é‡æ–°è¨ˆç®—è©²æ‰¹æ¬¡çµ±è¨ˆ
            await _sqlRunner.ExecuteAsync(
                "SELECT fn_mm_insert_metrics_by_batch(@datasetId, @batchId)",
                new { datasetId, batchId });
            
            // 3. æ›´æ–°æœ€çµ‚çµ±è¨ˆè¡¨ï¼ˆåƒ…å—å½±éŸ¿éƒ¨åˆ†ï¼‰
            await _sqlRunner.ExecuteAsync(
                "CALL sp_mm_upsert_final_for_affected(@datasetId, @batchId)",
                new { datasetId, batchId });
            
            // 4. æ¸…é™¤ç›¸é—œå¿«å–
            await _cacheService.RemoveByPatternAsync($"metrics:dataset:{datasetId}:*");
            
            // 5. æ›´æ–°æ‰¹æ¬¡ç‹€æ…‹
            await _datasetRepository.UpdateBatchStatusAsync(batchId, "Succeeded");
            
            _logger.LogInformation("å¢é‡æ›´æ–°å®Œæˆ: DatasetId={DatasetId}, BatchId={BatchId}", 
                datasetId, batchId);
        }
        catch (Exception ex)
        {
            await _datasetRepository.UpdateBatchStatusAsync(batchId, "Failed", ex.Message);
            _logger.LogError(ex, "å¢é‡æ›´æ–°å¤±æ•—: DatasetId={DatasetId}, BatchId={BatchId}", 
                datasetId, batchId);
            throw;
        }
    }
}
```

### è³‡æ–™ä¸€è‡´æ€§ä¿è­‰

1. **äº¤æ˜“æ§åˆ¶**ï¼šæ•´å€‹ ETL æµç¨‹åŒ…åœ¨è³‡æ–™åº«äº¤æ˜“ä¸­
2. **åŸå­æ“ä½œ**ï¼šæ‰¹æ¬¡è™•ç†æˆåŠŸå¾Œæ‰æ›´æ–°æœ€çµ‚çµ±è¨ˆè¡¨
3. **éŒ¯èª¤å›æ»¾**ï¼šè™•ç†å¤±æ•—æ™‚è‡ªå‹•å›æ»¾æ‰€æœ‰è®Šæ›´
4. **ç‹€æ…‹è¿½è¹¤**ï¼šé€é `dataset_batches.status` è¿½è¹¤è™•ç†ç‹€æ…‹
5. **å†ªç­‰æ€§ä¿è­‰**ï¼šåŒä¸€æ‰¹æ¬¡å¯å®‰å…¨é‡è¤‡åŸ·è¡Œï¼ˆå…ˆæ¸…ç†å¾Œé‡å»ºï¼‰

## ğŸ” èº«ä»½é©—è­‰

å°ˆæ¡ˆä½¿ç”¨ JWT é€²è¡Œèº«ä»½é©—è­‰ï¼š

```bash
# ç™»å…¥å–å¾— Token
curl -X POST https://localhost:5127/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email": "user@example.com", "password": "password"}'

# ä½¿ç”¨ Token å­˜å–å—ä¿è­·çš„ API
curl -X GET https://localhost:5127/api/metrics/kpi-summary/1 \
  -H "Authorization: Bearer <your-token>"
```

## ğŸ“Š ç›£æ§å’Œæ—¥èªŒ

### æ—¥èªŒä½ç½®
- **é–‹ç™¼ç’°å¢ƒ**: Console è¼¸å‡º
- **ç”Ÿç”¢ç’°å¢ƒ**: `logs/bidashboard-{date}.log`

### èƒŒæ™¯å·¥ä½œç›£æ§
- Hangfire Dashboard: `https://localhost:7000/hangfire`

### æ—¥èªŒç­‰ç´š
- **Debug**: è©³ç´°çš„é™¤éŒ¯è³‡è¨Š
- **Info**: ä¸€èˆ¬è³‡è¨Šå’Œæ¥­å‹™æ“ä½œ
- **Warning**: è­¦å‘Šè¨Šæ¯
- **Error**: éŒ¯èª¤å’Œç•°å¸¸

## ğŸ§ª æ¸¬è©¦

```bash
# åŸ·è¡Œå–®å…ƒæ¸¬è©¦
dotnet test

# åŸ·è¡Œç‰¹å®šæ¸¬è©¦å°ˆæ¡ˆ
dotnet test tests/BIDashboardBackend.Tests
```

## ğŸš€ éƒ¨ç½²

### ç”Ÿç”¢ç’°å¢ƒé…ç½®

é è¨­ä½¿ç”¨appsettings.jsonçš„è¨­å®šï¼Œå¦‚æœæœ‰ç’°å¢ƒè®Šæ•¸ï¼Œå°±ç”¨ç’°å¢ƒè®Šæ•¸è¦†è“‹ (å¯é¸)

1. è¨­å®šç’°å¢ƒè®Šæ•¸ï¼š
```bash
export ConnectionStrings__Pg="Host=prod-db;Port=5432;Database=bidb;Username=prod_user;Password=secure_password"
export Redis__ConnectionString="prod-redis:6379"
export JwtSettings__SecretKey="your-secure-secret-key"
```


### Docker éƒ¨ç½²

```bash
# ä½¿ç”¨ docker-compose
docker-compose up -d
```


